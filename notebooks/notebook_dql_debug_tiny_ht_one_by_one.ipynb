{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82a3b574",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-01T09:07:46.954258Z",
     "iopub.status.busy": "2023-02-01T09:07:46.953856Z",
     "iopub.status.idle": "2023-02-01T09:07:46.972471Z",
     "shell.execute_reply": "2023-02-01T09:07:46.971265Z"
    },
    "papermill": {
     "duration": 0.031424,
     "end_time": "2023-02-01T09:07:46.974952",
     "exception": false,
     "start_time": "2023-02-01T09:07:46.943528",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Notebook used for debugging purpose to train the\\nthe DQL agent and then run it one step at a time.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "\"\"\"Notebook used for debugging purpose to train the\n",
    "the DQL agent and then run it one step at a time.\n",
    "\"\"\"\n",
    "\n",
    "# pylint: disable=invalid-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "567d34c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-01T09:07:46.985887Z",
     "iopub.status.busy": "2023-02-01T09:07:46.985443Z",
     "iopub.status.idle": "2023-02-01T09:07:48.994811Z",
     "shell.execute_reply": "2023-02-01T09:07:48.994080Z"
    },
    "lines_to_next_cell": 2,
    "papermill": {
     "duration": 2.020002,
     "end_time": "2023-02-01T09:07:48.999367",
     "exception": false,
     "start_time": "2023-02-01T09:07:46.979365",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "import cyberbattle.agents.baseline.agent_wrapper as w\n",
    "import cyberbattle.agents.baseline.agent_dql as dqla\n",
    "import logging\n",
    "from cyberbattle.agents.baseline.agent_wrapper import ActionTrackingStateAugmentation, AgentWrapper, Verbosity\n",
    "from IPython.display import display\n",
    "import gym\n",
    "import yaml\n",
    "import json\n",
    "from cyberbattle.simulation.config import configuration, logger\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "981feb7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-01T09:07:49.013703Z",
     "iopub.status.busy": "2023-02-01T09:07:49.013094Z",
     "iopub.status.idle": "2023-02-01T09:07:49.022926Z",
     "shell.execute_reply": "2023-02-01T09:07:49.022211Z"
    },
    "papermill": {
     "duration": 0.019756,
     "end_time": "2023-02-01T09:07:49.025738",
     "exception": false,
     "start_time": "2023-02-01T09:07:49.005982",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "max_episode_steps = 50\n",
    "log_results = os.getenv(\"LOG_RESULTS\", 'False').lower() in ('true', '1', 't')\n",
    "gymid = os.getenv(\"GYMID\", 'CyberBattleTinyMicro-v0')\n",
    "log_level = os.getenv('LOG_LEVEL', \"info\")\n",
    "iteration_count = None\n",
    "honeytokens_on = None\n",
    "training_episode_count = None\n",
    "train_while_exploit = os.getenv(\"TRAIN_WHILE_EXPLOIT\", 'True').lower() in ('true', '1', 't')\n",
    "eval_episode_count = int(os.getenv('EVAL_EPISODE_COUNT', 0))\n",
    "eval_freq = int(os.getenv('EVAL_FREQ', 0))\n",
    "epsilon_exponential_decay = int(os.getenv('EPS_EXP_DECAY', max_episode_steps * 4000))  # 5000\n",
    "mean_reward_window = int(os.getenv('MEAN_REWARD_WINDOW', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da0ccbd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-01T09:07:49.038070Z",
     "iopub.status.busy": "2023-02-01T09:07:49.037649Z",
     "iopub.status.idle": "2023-02-01T09:07:49.041759Z",
     "shell.execute_reply": "2023-02-01T09:07:49.041049Z"
    },
    "papermill": {
     "duration": 0.01296,
     "end_time": "2023-02-01T09:07:49.044190",
     "exception": false,
     "start_time": "2023-02-01T09:07:49.031230",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gymid = \"CyberBattleTinyMicro-v1\"\n",
    "training_episode_count = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc6ef11e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-01T09:07:49.054204Z",
     "iopub.status.busy": "2023-02-01T09:07:49.053670Z",
     "iopub.status.idle": "2023-02-01T09:07:49.067828Z",
     "shell.execute_reply": "2023-02-01T09:07:49.067544Z"
    },
    "papermill": {
     "duration": 0.023388,
     "end_time": "2023-02-01T09:07:49.071586",
     "exception": false,
     "start_time": "2023-02-01T09:07:49.048198",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "iteration_count = max_episode_steps if iteration_count is None else iteration_count\n",
    "os.environ['TRAINING_EPISODE_COUNT'] = os.getenv('TRAINING_EPISODE_COUNT', 1000) if training_episode_count is None else str(training_episode_count)\n",
    "training_episode_count = int(os.environ['TRAINING_EPISODE_COUNT'])\n",
    "\n",
    "log_dir = '/logs/exper/' + \"notebook_dql_debug_with_tinymicro\"\n",
    "# convert the datetime object to string of specific format\n",
    "datetime_str = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_dir = os.path.join(log_dir, gymid, datetime_str)\n",
    "os.environ['LOG_DIR'] = log_dir\n",
    "\n",
    "os.environ['LOG_RESULTS'] = str(log_results).lower()\n",
    "exploit_train = \"exploittrain\" * train_while_exploit + \"exploitinfer\" * (1 - train_while_exploit)\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True) if log_results else ''\n",
    "\n",
    "configuration.update_globals(log_dir, gymid, log_level, log_results)\n",
    "configuration.update_logger()\n",
    "\n",
    "# if os.environ['RUN_IN_SILENT_MODE'] in ['true']:\n",
    "#     f = open(os.devnull, 'w')\n",
    "#     sys.stdout = f\n",
    "\n",
    "# progressbar.streams.wrap_stderr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e578da2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-01T09:07:49.086152Z",
     "iopub.status.busy": "2023-02-01T09:07:49.085752Z",
     "iopub.status.idle": "2023-02-01T09:07:49.104475Z",
     "shell.execute_reply": "2023-02-01T09:07:49.104202Z"
    },
    "lines_to_next_cell": 0,
    "papermill": {
     "duration": 0.028086,
     "end_time": "2023-02-01T09:07:49.107796",
     "exception": false,
     "start_time": "2023-02-01T09:07:49.079710",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ctf_env = gym.make(gymid)\n",
    "ep = w.EnvironmentBounds.of_identifiers(\n",
    "    maximum_node_count=ctf_env.bounds.maximum_node_count,  # either we identify from configuration, or by ourselves\n",
    "    maximum_total_credentials=1,\n",
    "    identifiers=ctf_env.identifiers\n",
    ")\n",
    "\n",
    "ctf_env = gym.make(gymid, env_bounds=ep)\n",
    "ctf_env.spec.max_episode_steps = max_episode_steps\n",
    "\n",
    "\n",
    "# if not log_results:\n",
    "#     lhStdout = logger.handlers[0]  # stdout is the only handler initially\n",
    "#     logger.removeHandler(lhStdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea68c82",
   "metadata": {
    "lines_to_next_cell": 0,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2023-02-01T09:07:49.115205",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate the Deep Q-learning agent\n",
    "\n",
    "os.makedirs(os.path.join(log_dir, 'training'), exist_ok=True) if log_results else ''\n",
    "env_config = json.loads(json.dumps(dotenv_values()))\n",
    "if configuration.log_results:\n",
    "    with open(os.path.join(log_dir, 'training', '.env.data.yml'), 'w') as outfile:\n",
    "        yaml.dump(env_config, outfile, default_flow_style=False)\n",
    "    with open(os.path.join(log_dir, 'training', '.env.data'), 'w') as outfile:\n",
    "        outfile.write(\"\\n\".join(k + \"=\" + str(v) for k, v in env_config.items()))\n",
    "    logger.info(f\"Loading env variables!\\n{str(env_config)}\")\n",
    "\n",
    "\n",
    "learning_rate = 0.01  # 0.01\n",
    "gamma = 0.015  # 0.015\n",
    "dqn_learning_run = learner.epsilon_greedy_search(\n",
    "    cyberbattle_gym_env=ctf_env,\n",
    "    environment_properties=ep,\n",
    "    learner=dqla.DeepQLearnerPolicy(\n",
    "        ep=ep,\n",
    "        gamma=gamma,\n",
    "        replay_memory_size=10000,\n",
    "        target_update=5,\n",
    "        batch_size=512,  # TODO increase?\n",
    "        learning_rate=learning_rate,  # torch default learning rate is 1e-2\n",
    "        train_while_exploit=train_while_exploit\n",
    "    ),\n",
    "    episode_count=training_episode_count,\n",
    "    iteration_count=iteration_count,\n",
    "    epsilon=0.90,\n",
    "    epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "    epsilon_minimum=0.10,\n",
    "    eval_episode_count=eval_episode_count,\n",
    "    eval_freq=eval_freq,\n",
    "    mean_reward_window=mean_reward_window,\n",
    "    verbosity=Verbosity.Quiet,\n",
    "    render=False,\n",
    "    render_last_episode_rewards_to=os.path.join(log_dir, 'training') if log_results else None,\n",
    "    plot_episodes_length=False,\n",
    "    title=\"DQL\",\n",
    "    save_model_filename=log_results * os.path.join(log_dir, 'training',\n",
    "                                                   f\"{exploit_train}_te{training_episode_count}.tar\")\n",
    ")\n",
    "\n",
    "if log_results:\n",
    "    configuration.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1e9b08",
   "metadata": {
    "lines_to_next_cell": 0,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the environment\n",
    "\n",
    "# current_o = ctf_env_2.reset()\n",
    "# wrapped_env = AgentWrapper(ctf_env_2, ActionTrackingStateAugmentation(ep, current_o))\n",
    "DQL_agent = dqn_learning_run['learner']\n",
    "logger.setLevel(logging.INFO) if log_results else ''\n",
    "\n",
    "if log_results:\n",
    "    logger.info(\"Saving model to directory \" + log_dir)\n",
    "    DQL_agent.save(os.path.join(log_dir, f\"{exploit_train}_te{training_episode_count}_final.tar\"))\n",
    "\n",
    "\n",
    "logger.info(\"\")\n",
    "logger.info(\"Now evaluate trained network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94794f08",
   "metadata": {
    "lines_to_next_cell": 0,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the trained agent to run the steps one by one\n",
    "\n",
    "max_steps = iteration_count\n",
    "verbosity = Verbosity.Normal\n",
    "DQL_agent.load_best(os.path.join(log_dir, 'training'))\n",
    "DQL_agent.train_while_exploit = train_while_exploit\n",
    "DQL_agent.policy_net.eval()\n",
    "\n",
    "current_o = ctf_env.reset()\n",
    "wrapped_env = AgentWrapper(ctf_env, ActionTrackingStateAugmentation(ep, current_o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135b74f1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate DQL agent 10 times\n",
    "for n_trial in range(10):\n",
    "    # next action suggested by DQL agent\n",
    "    h = []\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    df = None\n",
    "    current_o = wrapped_env.reset()\n",
    "    for i in range(max_steps):\n",
    "        logger.info(f\"Step {i}\")\n",
    "        if done:\n",
    "            break\n",
    "        # run the suggested action\n",
    "        action_style, next_action, _ = DQL_agent.exploit(wrapped_env, current_o)\n",
    "\n",
    "        if next_action is None:\n",
    "            logger.info(f\"Inference ended with error: next action == None, returned with aciton_style {action_style}\")\n",
    "            break\n",
    "        current_o, reward, done, info = wrapped_env.step(next_action)\n",
    "        total_reward += reward\n",
    "        action_str, reward_str = wrapped_env.internal_action_to_pretty_print(next_action, output_reward_str=True)\n",
    "        h.append((i,  # wrapped_env.get_explored_network_node_properties_bitmap_as_numpy(current_o),\n",
    "                  reward, total_reward,\n",
    "                  action_str, action_style, info['precondition_str'], info['profile_str'], info[\"reward_string\"]))  # \"\\t action  validity: \" +\n",
    "\n",
    "        df = pd.DataFrame(h, columns=[\"Step\", \"Reward\", \"Cumulative Reward\", \"Next action\", \"Processed by\", \"Precondition\", \"Profile\", \"Reward string\"])\n",
    "        df.set_index(\"Step\", inplace=True)\n",
    "        if log_results:\n",
    "            df.to_csv(os.path.join(log_dir, f'{exploit_train}_evaln{n_trial}_te{training_episode_count}_actions.csv'))\n",
    "\n",
    "    print(f'len: {len(h)}, total reward: {total_reward}')\n",
    "    pd.set_option(\"max_colwidth\", 10**3)\n",
    "    if df is not None:\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06b69ec1",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "title": "if not log_results else 'human'w"
   },
   "outputs": [],
   "source": [
    "    wrapped_env.render(mode='rgb_array', filename=None if not log_results else\n",
    "                       os.path.join(log_dir, f'{exploit_train}_evaln{n_trial}_te{training_episode_count}_network.png'))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,title,-all",
   "cell_metadata_json": true,
   "kernelspec": {
    "display_name": "python3",
    "language": "python",
    "name": "python3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "-",
   "output_path": "notebooks/notebook_dql_debug_tiny_ht_one_by_one.ipynb",
   "parameters": {
    "gymid": "CyberBattleTinyMicro-v1",
    "training_episode_count": 2000
   },
   "start_time": "2023-02-01T09:07:45.318335",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}