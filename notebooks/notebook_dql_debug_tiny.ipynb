{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b47ab74f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-18T09:55:35.835280Z",
     "iopub.status.busy": "2023-01-18T09:55:35.832586Z",
     "iopub.status.idle": "2023-01-18T09:55:35.905427Z",
     "shell.execute_reply": "2023-01-18T09:55:35.902074Z"
    },
    "papermill": {
     "duration": 0.109383,
     "end_time": "2023-01-18T09:55:35.920538",
     "exception": false,
     "start_time": "2023-01-18T09:55:35.811155",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Notebook used for debugging purpose to train the\\nthe DQL agent and then run it one step at a time.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "\"\"\"Notebook used for debugging purpose to train the\n",
    "the DQL agent and then run it one step at a time.\n",
    "\"\"\"\n",
    "\n",
    "# pylint: disable=invalid-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ee2a856",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-18T09:55:35.970561Z",
     "iopub.status.busy": "2023-01-18T09:55:35.967855Z",
     "iopub.status.idle": "2023-01-18T09:55:40.762780Z",
     "shell.execute_reply": "2023-01-18T09:55:40.761258Z"
    },
    "lines_to_next_cell": 2,
    "papermill": {
     "duration": 4.831434,
     "end_time": "2023-01-18T09:55:40.767458",
     "exception": false,
     "start_time": "2023-01-18T09:55:35.936024",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "import cyberbattle.agents.baseline.agent_wrapper as w\n",
    "import cyberbattle.agents.baseline.agent_dql as dqla\n",
    "import logging\n",
    "from cyberbattle.agents.baseline.agent_wrapper import ActionTrackingStateAugmentation, AgentWrapper, Verbosity\n",
    "from IPython.display import display\n",
    "import progressbar\n",
    "import gym\n",
    "from cyberbattle.simulation.config import configuration, logger\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "281fd4ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-18T09:55:40.797458Z",
     "iopub.status.busy": "2023-01-18T09:55:40.794968Z",
     "iopub.status.idle": "2023-01-18T09:55:40.811089Z",
     "shell.execute_reply": "2023-01-18T09:55:40.809304Z"
    },
    "papermill": {
     "duration": 0.032314,
     "end_time": "2023-01-18T09:55:40.815012",
     "exception": false,
     "start_time": "2023-01-18T09:55:40.782698",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "max_episode_steps = 50\n",
    "log_results = os.getenv(\"LOG_RESULTS\", 'False').lower() in ('true', '1', 't')\n",
    "gymid = os.getenv(\"GYMID\", 'CyberBattleTinyMicro-v0')\n",
    "log_level = os.getenv('LOG_LEVEL', \"info\")\n",
    "iteration_count = None\n",
    "training_episode_count = None\n",
    "train_while_exploit = os.getenv(\"TRAIN_WHILE_EXPLOIT\", 'True').lower() in ('true', '1', 't')\n",
    "exploit_train = \"exploit_train\"   # \"exploit_manual\"\n",
    "eval_episode_count = int(os.getenv('EVAL_EPISODE_COUNT', 0))\n",
    "eval_freq = int(os.getenv('EVAL_FREQ', 0))\n",
    "epsilon_exponential_decay = int(os.getenv('EPS_EXP_DECAY', max_episode_steps * 4000))  # 5000\n",
    "mean_reward_window = int(os.getenv('MEAN_REWARD_WINDOW', 10))\n",
    "\n",
    "log_dir = 'logs/exper/' + \"notebook_dql_debug_with_tinymicro\"\n",
    "# convert the datetime object to string of specific format\n",
    "datetime_str = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_dir = os.path.join(log_dir, gymid, datetime_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b035e3ab",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-18T09:55:40.827806Z",
     "iopub.status.busy": "2023-01-18T09:55:40.826223Z",
     "iopub.status.idle": "2023-01-18T09:55:40.833492Z",
     "shell.execute_reply": "2023-01-18T09:55:40.831998Z"
    },
    "papermill": {
     "duration": 0.017855,
     "end_time": "2023-01-18T09:55:40.837011",
     "exception": false,
     "start_time": "2023-01-18T09:55:40.819156",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gymid = \"CyberBattleTinyMicro-v1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d85c23cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-18T09:55:40.849667Z",
     "iopub.status.busy": "2023-01-18T09:55:40.848979Z",
     "iopub.status.idle": "2023-01-18T09:55:40.862204Z",
     "shell.execute_reply": "2023-01-18T09:55:40.860551Z"
    },
    "papermill": {
     "duration": 0.02447,
     "end_time": "2023-01-18T09:55:40.866113",
     "exception": false,
     "start_time": "2023-01-18T09:55:40.841643",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<progressbar.utils.WrappingIO at 0x7f5609e13c10>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iteration_count = max_episode_steps if iteration_count is None else iteration_count\n",
    "os.environ['TRAINING_EPISODE_COUNT'] = os.getenv('TRAINING_EPISODE_COUNT', 1000) if training_episode_count is None else training_episode_count\n",
    "training_episode_count = int(os.environ['TRAINING_EPISODE_COUNT'])\n",
    "os.environ['LOG_DIR'] = log_dir\n",
    "os.environ['LOG_RESULTS'] = str(log_results).lower()\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True) if log_results else ''\n",
    "\n",
    "configuration.update_globals(log_dir, gymid, log_level, log_results)\n",
    "configuration.update_logger()\n",
    "\n",
    "# if os.environ['RUN_IN_SILENT_MODE'] in ['true']:\n",
    "#     f = open(os.devnull, 'w')\n",
    "#     sys.stdout = f\n",
    "\n",
    "progressbar.streams.wrap_stderr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90b358b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-18T09:55:40.878637Z",
     "iopub.status.busy": "2023-01-18T09:55:40.877151Z",
     "iopub.status.idle": "2023-01-18T09:55:40.895257Z",
     "shell.execute_reply": "2023-01-18T09:55:40.893993Z"
    },
    "lines_to_next_cell": 0,
    "papermill": {
     "duration": 0.028415,
     "end_time": "2023-01-18T09:55:40.898922",
     "exception": false,
     "start_time": "2023-01-18T09:55:40.870507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ctf_env = gym.make(gymid)\n",
    "ep = w.EnvironmentBounds.of_identifiers(\n",
    "    maximum_node_count=ctf_env.bounds.maximum_node_count,  # either we identify from configuration, or by ourselves\n",
    "    maximum_total_credentials=1,\n",
    "    identifiers=ctf_env.identifiers\n",
    ")\n",
    "\n",
    "ctf_env = gym.make(gymid, env_bounds=ep)\n",
    "ctf_env.spec.max_episode_steps = max_episode_steps\n",
    "\n",
    "\n",
    "# if not log_results:\n",
    "#     lhStdout = logger.handlers[0]  # stdout is the only handler initially\n",
    "#     logger.removeHandler(lhStdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74a0e7eb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-18T09:55:40.918008Z",
     "iopub.status.busy": "2023-01-18T09:55:40.915560Z",
     "iopub.status.idle": "2023-01-18T09:57:24.821434Z",
     "shell.execute_reply": "2023-01-18T09:57:24.818939Z"
    },
    "lines_to_next_cell": 0,
    "papermill": {
     "duration": 103.181087,
     "end_time": "2023-01-18T09:57:24.085374",
     "exception": false,
     "start_time": "2023-01-18T09:55:40.904287",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "###### DQL\n",
      "Learning with: episode_count=3000,iteration_count=50,ϵ=0.9,ϵ_min=0.1, ϵ_expdecay=250000,γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n",
      "  ## Episode: 1/3000 'DQL' ϵ=0.9000, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 ended at t=50 total_reward -1130.0 with \n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/41 (0.05)\n",
      "    explore-remote: 0/2 (0.00)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/5 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 2/3000 'DQL' ϵ=0.8998, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 2 ended at t=50 total_reward -1101.0 with \n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/42 (0.05)\n",
      "    explore-remote: 0/2 (0.00)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/4 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 3/3000 'DQL' ϵ=0.8997, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 3 ended at t=50 total_reward -1032.0 with \n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/41 (0.05)\n",
      "    explore-remote: 1/4 (0.20)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/2 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 4/3000 'DQL' ϵ=0.8995, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 4 ended at t=50 total_reward -1130.0 with \n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/42 (0.05)\n",
      "    explore-remote: 0/1 (0.00)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/5 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 5/3000 'DQL' ϵ=0.8994, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5 ended at t=50 total_reward -1207.0 with \n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/39 (0.05)\n",
      "    explore-remote: 0/1 (0.00)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/8 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 6/3000 'DQL' ϵ=0.8992, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 6 ended at t=50 total_reward -1101.0 with \n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/42 (0.05)\n",
      "    explore-remote: 0/2 (0.00)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/4 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 7/3000 'DQL' ϵ=0.8990, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 7 ended at t=50 total_reward -1092.0 with \n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/43 (0.04)\n",
      "    explore-remote: 0/2 (0.00)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/3 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 8/3000 'DQL' ϵ=0.8989, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8 ended at t=50 total_reward -1111.0 with \n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/42 (0.05)\n",
      "    explore-remote: 0/2 (0.00)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/4 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 9/3000 'DQL' ϵ=0.8987, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 9 ended at t=50 total_reward -1246.0 with \n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/38 (0.05)\n",
      "    explore-remote: 0/1 (0.00)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/9 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 10/3000 'DQL' ϵ=0.8986, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 ended at t=50 total_reward -1137.0 with \n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/39 (0.05)\n",
      "    explore-remote: 1/1 (0.50)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/7 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 11/3000 'DQL' ϵ=0.8984, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 11 ended at t=50 total_reward -1139.0 with loss=5.314572334289551\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/42 (0.05)\n",
      "    explore-remote: 0/0 (NaN)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/6 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 12/3000 'DQL' ϵ=0.8982, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 12 ended at t=50 total_reward -1120.0 with loss=3.390625\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/42 (0.05)\n",
      "    explore-remote: 0/1 (0.00)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/5 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 13/3000 'DQL' ϵ=0.8981, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 13 ended at t=50 total_reward -1234.0 with loss=2.547646999359131\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/35 (0.05)\n",
      "    explore-remote: 1/2 (0.33)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/10 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 14/3000 'DQL' ϵ=0.8979, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 14 ended at t=50 total_reward -994.0 with loss=2.026846408843994\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/47 (0.04)\n",
      "    explore-remote: 0/0 (NaN)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/1 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 15/3000 'DQL' ϵ=0.8978, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 15 ended at t=50 total_reward -1140.0 with loss=2.1726200580596924\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/40 (0.05)\n",
      "    explore-remote: 0/3 (0.00)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/5 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 16/3000 'DQL' ϵ=0.8976, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 16 ended at t=50 total_reward -1130.0 with loss=1.9407927989959717\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 1/41 (0.02)\n",
      "    explore-remote: 0/2 (0.00)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 1/0 (1.00)\n",
      "    exploit-remote: 0/5 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 17/3000 'DQL' ϵ=0.8974, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 17 ended at t=50 total_reward -1101.0 with loss=1.8770813941955566\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/43 (0.04)\n",
      "    explore-remote: 0/1 (0.00)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/4 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 18/3000 'DQL' ϵ=0.8973, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 18 ended at t=50 total_reward -1023.0 with loss=2.111839771270752\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 1/38 (0.03)\n",
      "    explore-remote: 2/5 (0.29)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 1/0 (1.00)\n",
      "    exploit-remote: 0/3 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 19/3000 'DQL' ϵ=0.8971, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 19 ended at t=50 total_reward -1091.0 with loss=2.176680326461792\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/43 (0.04)\n",
      "    explore-remote: 0/1 (0.00)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/4 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 20/3000 'DQL' ϵ=0.8970, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20 ended at t=50 total_reward -1140.0 with loss=1.9754343032836914\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 1/41 (0.02)\n",
      "    explore-remote: 0/2 (0.00)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 1/0 (1.00)\n",
      "    exploit-remote: 0/5 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 21/3000 'DQL' ϵ=0.8968, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 21 ended at t=50 total_reward -1160.0 with loss=1.7048293352127075\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/40 (0.05)\n",
      "    explore-remote: 0/3 (0.00)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/5 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 22/3000 'DQL' ϵ=0.8967, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 22 ended at t=50 total_reward -1081.0 with loss=1.8823269605636597\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 1/44 (0.02)\n",
      "    explore-remote: 0/0 (NaN)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 1/0 (1.00)\n",
      "    exploit-remote: 0/4 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 23/3000 'DQL' ϵ=0.8965, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 23 ended at t=50 total_reward -1168.0 with loss=1.7933176755905151\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/41 (0.05)\n",
      "    explore-remote: 0/0 (NaN)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/7 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 24/3000 'DQL' ϵ=0.8963, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 24 ended at t=50 total_reward -1111.0 with loss=1.5829453468322754\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/41 (0.05)\n",
      "    explore-remote: 0/3 (0.00)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/4 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 25/3000 'DQL' ϵ=0.8962, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 25 ended at t=50 total_reward -1099.0 with loss=1.4114421606063843\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/39 (0.05)\n",
      "    explore-remote: 1/3 (0.25)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/5 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 26/3000 'DQL' ϵ=0.8960, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 26 ended at t=50 total_reward -1188.0 with loss=1.2944681644439697\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/40 (0.05)\n",
      "    explore-remote: 0/1 (0.00)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/7 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 27/3000 'DQL' ϵ=0.8959, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 27 ended at t=50 total_reward -1139.0 with loss=1.2660026550292969\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/42 (0.05)\n",
      "    explore-remote: 0/0 (NaN)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/6 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 28/3000 'DQL' ϵ=0.8957, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 28 ended at t=50 total_reward -1188.0 with loss=1.4521808624267578\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/39 (0.05)\n",
      "    explore-remote: 0/1 (0.00)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/8 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 29/3000 'DQL' ϵ=0.8955, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 29 ended at t=50 total_reward -995.0 with loss=1.3645192384719849\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/45 (0.04)\n",
      "    explore-remote: 0/2 (0.00)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/1 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 30/3000 'DQL' ϵ=0.8954, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 30 ended at t=50 total_reward -1120.0 with loss=1.2996902465820312\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 1/42 (0.02)\n",
      "    explore-remote: 0/1 (0.00)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 1/0 (1.00)\n",
      "    exploit-remote: 0/5 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 31/3000 'DQL' ϵ=0.8952, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 31 ended at t=50 total_reward -1111.0 with loss=1.3280991315841675\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 1/41 (0.02)\n",
      "    explore-remote: 0/3 (0.00)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 1/0 (1.00)\n",
      "    exploit-remote: 0/4 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 32/3000 'DQL' ϵ=0.8951, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 32 ended at t=50 total_reward -1196.0 with loss=1.3655550479888916\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/35 (0.05)\n",
      "    explore-remote: 1/4 (0.20)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/8 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 33/3000 'DQL' ϵ=0.8949, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 33 ended at t=50 total_reward -1053.0 with loss=1.1139122247695923\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 1/44 (0.02)\n",
      "    explore-remote: 0/2 (0.00)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 1/0 (1.00)\n",
      "    exploit-remote: 0/2 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 34/3000 'DQL' ϵ=0.8947, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 34 ended at t=50 total_reward -1188.0 with loss=1.4205697774887085\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/40 (0.05)\n",
      "    explore-remote: 0/1 (0.00)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/7 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 35/3000 'DQL' ϵ=0.8946, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 35 ended at t=50 total_reward -1110.0 with loss=1.197160243988037\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/43 (0.04)\n",
      "    explore-remote: 0/0 (NaN)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/5 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 36/3000 'DQL' ϵ=0.8944, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 36 ended at t=50 total_reward -1110.0 with loss=1.3628863096237183\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/43 (0.04)\n",
      "    explore-remote: 0/0 (NaN)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/5 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 37/3000 'DQL' ϵ=0.8943, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 37 ended at t=50 total_reward -1121.0 with loss=1.1552034616470337\n",
      "  Breakdown [Reward/NoReward (Success rate)]\n",
      "    explore-local: 2/41 (0.05)\n",
      "    explore-remote: 0/3 (0.00)\n",
      "    explore-connect: 0/0 (NaN)\n",
      "    exploit-local: 0/0 (NaN)\n",
      "    exploit-remote: 0/4 (0.00)\n",
      "    exploit-connect: 0/0 (NaN)\n",
      "  exploit deflected to exploration: 0\n",
      "  ## Episode: 38/3000 'DQL' ϵ=0.8941, γ=0.015, lr=0.01, replaymemory=10000,\n",
      "batch=512, target_update=5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Deep Q-learning agent\n",
    "\n",
    "os.makedirs(os.path.join(log_dir, 'training'), exist_ok=True) if log_results else ''\n",
    "\n",
    "learning_rate = 0.01  # 0.01\n",
    "gamma = 0.015  # 0.015\n",
    "dqn_learning_run = learner.epsilon_greedy_search(\n",
    "    cyberbattle_gym_env=ctf_env,\n",
    "    environment_properties=ep,\n",
    "    learner=dqla.DeepQLearnerPolicy(\n",
    "        ep=ep,\n",
    "        gamma=gamma,\n",
    "        replay_memory_size=10000,\n",
    "        target_update=5,\n",
    "        batch_size=512,  # TODO increase?\n",
    "        learning_rate=learning_rate  # torch default learning rate is 1e-2\n",
    "    ),\n",
    "    episode_count=training_episode_count,\n",
    "    iteration_count=iteration_count,\n",
    "    epsilon=0.90,\n",
    "    epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "    epsilon_minimum=0.10,\n",
    "    eval_episode_count=eval_episode_count,\n",
    "    eval_freq=eval_freq,\n",
    "    mean_reward_window=mean_reward_window,\n",
    "    verbosity=Verbosity.Quiet,\n",
    "    render=False,\n",
    "    render_last_episode_rewards_to=os.path.join(log_dir, 'training/') if log_results else None,\n",
    "    plot_episodes_length=False,\n",
    "    title=\"DQL\",\n",
    "    save_model_filename=log_results * os.path.join(os.path.join(log_dir, 'training/'),\n",
    "                                                   f\"{exploit_train}_{train_while_exploit * 'train_while_exploit'}_trainepisodes{training_episode_count}_best_model.tar\")\n",
    ")\n",
    "\n",
    "if log_results:\n",
    "    configuration.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23367ab4",
   "metadata": {
    "lines_to_next_cell": 2,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the environment\n",
    "\n",
    "# current_o = ctf_env_2.reset()\n",
    "# wrapped_env = AgentWrapper(ctf_env_2, ActionTrackingStateAugmentation(ep, current_o))\n",
    "DQL_agent = dqn_learning_run['learner']\n",
    "logger.setLevel(logging.INFO) if log_results else ''\n",
    "\n",
    "if log_results:\n",
    "    logger.info(\"Saving model to directory \" + log_dir)\n",
    "    DQL_agent.save(os.path.join(log_dir, f\"{exploit_train}_{train_while_exploit*'train_while_exploit'}_trainepisodes{training_episode_count}_checkpoint.tar\"))\n",
    "\n",
    "\n",
    "logger.info(\"\")\n",
    "logger.info(\"Now evaluate trained network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984854fd",
   "metadata": {
    "lines_to_next_cell": 0,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the trained agent to run the steps one by one\n",
    "\n",
    "max_steps = iteration_count\n",
    "verbosity = Verbosity.Normal\n",
    "DQL_agent.load_best(os.path.join(log_dir, 'training'))\n",
    "DQL_agent.train_while_exploit = train_while_exploit\n",
    "DQL_agent.policy_net.eval()\n",
    "\n",
    "current_o = ctf_env.reset()\n",
    "wrapped_env = AgentWrapper(ctf_env, ActionTrackingStateAugmentation(ep, current_o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71824c32",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate DQL agent 10 times\n",
    "for n_trial in range(10):\n",
    "    # next action suggested by DQL agent\n",
    "    h = []\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    df = None\n",
    "    current_o = wrapped_env.reset()\n",
    "    for i in range(max_steps):\n",
    "        logger.info(f\"Step {i}\")\n",
    "        if done:\n",
    "            break\n",
    "        # run the suggested action\n",
    "        action_style, next_action, _ = DQL_agent.exploit(wrapped_env, current_o)\n",
    "\n",
    "        if next_action is None:\n",
    "            logger.info(f\"Inference ended with error: next action == None, returned with aciton_style {action_style}\")\n",
    "            break\n",
    "        current_o, reward, done, info = wrapped_env.step(next_action)\n",
    "        total_reward += reward\n",
    "        action_str, reward_str = wrapped_env.internal_action_to_pretty_print(next_action, output_reward_str=True)\n",
    "        h.append((i,  # wrapped_env.get_explored_network_node_properties_bitmap_as_numpy(current_o),\n",
    "                  reward, total_reward,\n",
    "                  action_str, action_style, info['precondition_str'], info['profile_str'], info[\"reward_string\"]))  # \"\\t action  validity: \" +\n",
    "\n",
    "        df = pd.DataFrame(h, columns=[\"Step\", \"Reward\", \"Cumulative Reward\", \"Next action\", \"Processed by\", \"Precondition\", \"Profile\", \"Reward string\"])\n",
    "        df.set_index(\"Step\", inplace=True)\n",
    "        if log_results:\n",
    "            df.to_csv(os.path.join(log_dir, f'{exploit_train}_{train_while_exploit*\"train_while_exploit\"}_trial{n_trial}_trainepisodes{training_episode_count}_output.csv'))\n",
    "\n",
    "    print(f'len: {len(h)}, total reward: {total_reward}')\n",
    "    pd.set_option(\"max_colwidth\", 10**3)\n",
    "    if df is not None:\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a41655",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": [],
    "title": "if not log_results else 'human'w"
   },
   "outputs": [],
   "source": [
    "    wrapped_env.render(mode='rgb_array', filename=None if not log_results else\n",
    "                       os.path.join(log_dir, f'{exploit_train}_{train_while_exploit*\"train_while_exploit\"}_trial{n_trial}_trainepisodes{training_episode_count}_discovered_network.png'))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,title,-all",
   "cell_metadata_json": true,
   "kernelspec": {
    "display_name": "python3",
    "language": "python",
    "name": "python3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 112.630619,
   "end_time": "2023-01-18T09:57:24.827585",
   "environment_variables": {},
   "exception": null,
   "input_path": "-",
   "output_path": "notebooks/notebook_dql_debug_tiny.ipynb",
   "parameters": {
    "gymid": "CyberBattleTinyMicro-v1"
   },
   "start_time": "2023-01-18T09:55:32.196966",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}