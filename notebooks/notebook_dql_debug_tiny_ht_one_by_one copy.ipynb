{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9523662",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-01T17:16:13.355527Z",
     "iopub.status.busy": "2023-02-01T17:16:13.354788Z",
     "iopub.status.idle": "2023-02-01T17:16:13.376200Z",
     "shell.execute_reply": "2023-02-01T17:16:13.375448Z"
    },
    "papermill": {
     "duration": 0.034941,
     "end_time": "2023-02-01T17:16:13.379556",
     "exception": false,
     "start_time": "2023-02-01T17:16:13.344615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Notebook used for debugging purpose to train the\\nthe DQL agent and then run it one step at a time.\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "\"\"\"Notebook used for debugging purpose to train the\n",
    "the DQL agent and then run it one step at a time.\n",
    "\"\"\"\n",
    "\n",
    "# pylint: disable=invalid-name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e311a7ef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-01T17:16:13.391424Z",
     "iopub.status.busy": "2023-02-01T17:16:13.390920Z",
     "iopub.status.idle": "2023-02-01T17:16:15.511497Z",
     "shell.execute_reply": "2023-02-01T17:16:15.510772Z"
    },
    "lines_to_next_cell": 2,
    "papermill": {
     "duration": 2.131092,
     "end_time": "2023-02-01T17:16:15.516053",
     "exception": false,
     "start_time": "2023-02-01T17:16:13.384961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "import cyberbattle.agents.baseline.agent_wrapper as w\n",
    "import cyberbattle.agents.baseline.agent_dql as dqla\n",
    "import logging\n",
    "from cyberbattle.agents.baseline.agent_wrapper import ActionTrackingStateAugmentation, AgentWrapper, Verbosity\n",
    "from IPython.display import display\n",
    "import gym\n",
    "import yaml\n",
    "import json\n",
    "from cyberbattle.simulation.config import configuration, logger\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86a7b028",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-01T17:16:15.530367Z",
     "iopub.status.busy": "2023-02-01T17:16:15.529745Z",
     "iopub.status.idle": "2023-02-01T17:16:15.540249Z",
     "shell.execute_reply": "2023-02-01T17:16:15.539601Z"
    },
    "papermill": {
     "duration": 0.018689,
     "end_time": "2023-02-01T17:16:15.542932",
     "exception": false,
     "start_time": "2023-02-01T17:16:15.524243",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "max_episode_steps = 50\n",
    "log_results = os.getenv(\"LOG_RESULTS\", 'False').lower() in ('true', '1', 't')\n",
    "gymid = os.getenv(\"GYMID\", 'CyberBattleTinyMicro-v0')\n",
    "log_level = os.getenv('LOG_LEVEL', \"info\")\n",
    "iteration_count = None\n",
    "honeytokens_on = None\n",
    "training_episode_count = None\n",
    "train_while_exploit = os.getenv(\"TRAIN_WHILE_EXPLOIT\", 'True').lower() in ('true', '1', 't')\n",
    "eval_episode_count = int(os.getenv('EVAL_EPISODE_COUNT', 0))\n",
    "eval_freq = int(os.getenv('EVAL_FREQ', 0))\n",
    "epsilon_exponential_decay = int(os.getenv('EPS_EXP_DECAY', max_episode_steps * 4000))  # 5000\n",
    "mean_reward_window = int(os.getenv('MEAN_REWARD_WINDOW', 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ce14e06",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-01T17:16:15.553133Z",
     "iopub.status.busy": "2023-02-01T17:16:15.552811Z",
     "iopub.status.idle": "2023-02-01T17:16:15.556682Z",
     "shell.execute_reply": "2023-02-01T17:16:15.555904Z"
    },
    "papermill": {
     "duration": 0.011253,
     "end_time": "2023-02-01T17:16:15.559074",
     "exception": false,
     "start_time": "2023-02-01T17:16:15.547821",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gymid = \"CyberBattleTinyMicro-v2\"\n",
    "training_episode_count = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b45fa8b9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-01T17:16:15.568792Z",
     "iopub.status.busy": "2023-02-01T17:16:15.568374Z",
     "iopub.status.idle": "2023-02-01T17:16:15.581719Z",
     "shell.execute_reply": "2023-02-01T17:16:15.581306Z"
    },
    "papermill": {
     "duration": 0.021632,
     "end_time": "2023-02-01T17:16:15.584732",
     "exception": false,
     "start_time": "2023-02-01T17:16:15.563100",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "iteration_count = max_episode_steps if iteration_count is None else iteration_count\n",
    "os.environ['TRAINING_EPISODE_COUNT'] = os.getenv('TRAINING_EPISODE_COUNT', 1000) if training_episode_count is None else str(training_episode_count)\n",
    "training_episode_count = int(os.environ['TRAINING_EPISODE_COUNT'])\n",
    "\n",
    "log_dir = '/logs/exper/' + \"notebook_dql_debug_with_tinymicro\"\n",
    "# convert the datetime object to string of specific format\n",
    "datetime_str = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_dir = os.path.join(log_dir, gymid, datetime_str)\n",
    "os.environ['LOG_DIR'] = log_dir\n",
    "\n",
    "os.environ['LOG_RESULTS'] = str(log_results).lower()\n",
    "exploit_train = \"exploittrain\" * train_while_exploit + \"exploitinfer\" * (1 - train_while_exploit)\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True) if log_results else ''\n",
    "\n",
    "configuration.update_globals(log_dir, gymid, log_level, log_results)\n",
    "configuration.update_logger()\n",
    "\n",
    "# if os.environ['RUN_IN_SILENT_MODE'] in ['true']:\n",
    "#     f = open(os.devnull, 'w')\n",
    "#     sys.stdout = f\n",
    "\n",
    "# progressbar.streams.wrap_stderr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0a84b64",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-01T17:16:15.597369Z",
     "iopub.status.busy": "2023-02-01T17:16:15.597044Z",
     "iopub.status.idle": "2023-02-01T17:16:15.614773Z",
     "shell.execute_reply": "2023-02-01T17:16:15.614494Z"
    },
    "lines_to_next_cell": 0,
    "papermill": {
     "duration": 0.0271,
     "end_time": "2023-02-01T17:16:15.617527",
     "exception": false,
     "start_time": "2023-02-01T17:16:15.590427",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ctf_env = gym.make(gymid)\n",
    "ep = w.EnvironmentBounds.of_identifiers(\n",
    "    maximum_node_count=ctf_env.bounds.maximum_node_count,  # either we identify from configuration, or by ourselves\n",
    "    maximum_total_credentials=1,\n",
    "    identifiers=ctf_env.identifiers\n",
    ")\n",
    "\n",
    "ctf_env = gym.make(gymid, env_bounds=ep)\n",
    "ctf_env.spec.max_episode_steps = max_episode_steps\n",
    "\n",
    "\n",
    "# if not log_results:\n",
    "#     lhStdout = logger.handlers[0]  # stdout is the only handler initially\n",
    "#     logger.removeHandler(lhStdout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69c4f0a1",
   "metadata": {
    "lines_to_next_cell": 0,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": false,
     "start_time": "2023-02-01T17:16:15.624661",
     "status": "running"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode    1|Iteration 0|reward: -------|last_reward_at: --|done_at: --|epsilon: -----|best_eval_mean: ------|Elapsed Time: 0:00:00|ETA:  --:--:--||\n",
      "Episode    1|Iteration 1|reward:    22.0|last_reward_at: --|done_at: --|epsilon: -----|best_eval_mean: ------|Elapsed Time: 0:00:00|ETA:   0:00:00||\n",
      "Episode    1|Iteration 1|reward:    22.0|last_reward_at: --|done_at: --|epsilon:   0.9|best_eval_mean: ------|Elapsed Time: 0:00:00|ETA:   0:00:00||\n",
      "Episode    1|Iteration 1|reward:    22.0|last_reward_at: --|done_at: --|epsilon:   0.9|best_eval_mean: -1.79769e+308|Elapsed Time: 0:00:00|ETA:   0:00:00||\n",
      "Episode    1|Iteration 1|reward:    22.0|last_reward_at:  1|done_at: --|epsilon:   0.9|best_eval_mean: -1.79769e+308|Elapsed Time: 0:00:00|ETA:   0:00:00||\n",
      "Episode    1|Iteration 2|reward:    54.0|last_reward_at:  1|done_at: --|epsilon:   0.9|best_eval_mean: -1.79769e+308|Elapsed Time: 0:00:00|ETA:   0:00:00||\n",
      "Episode    1|Iteration 2|reward:    54.0|last_reward_at:  1|done_at: --|epsilon:   0.9|best_eval_mean: -1.79769e+308|Elapsed Time: 0:00:00|ETA:   0:00:00||\n",
      "Episode    1|Iteration 2|reward:    54.0|last_reward_at:  2|done_at: --|epsilon:   0.9|best_eval_mean: -1.79769e+308|Elapsed Time: 0:00:00|ETA:   0:00:00||\n",
      "Episode    1|Iteration 3|reward:    33.0|last_reward_at:  2|done_at: --|epsilon:   0.9|best_eval_mean: -1.79769e+308|Elapsed Time: 0:00:00|ETA:   0:00:00||\n",
      "Episode    1|Iteration 3|reward:    33.0|last_reward_at:  2|done_at: --|epsilon:   0.9|best_eval_mean: -1.79769e+308|Elapsed Time: 0:00:00|ETA:   0:00:00||\n",
      "Episode    1|Iteration 4|reward:    12.0|last_reward_at:  2|done_at: --|epsilon:   0.9|best_eval_mean: -1.79769e+308|Elapsed Time: 0:00:00|ETA:   0:00:00||\n",
      "Episode    1|Iteration 4|reward:    12.0|last_reward_at:  2|done_at: --|epsilon:   0.9|best_eval_mean: -1.79769e+308|Elapsed Time: 0:00:00|ETA:   0:00:00||\n",
      "Episode    1|Iteration 5|reward:    -9.0|last_reward_at:  2|done_at: --|epsilon:   0.9|best_eval_mean: -1.79769e+308|Elapsed Time: 0:00:00|ETA:   0:00:00||\n",
      "Episode    1|Iteration 5|reward:    -9.0|last_reward_at:  2|done_at: --|epsilon:   0.9|best_eval_mean: -1.79769e+308|Elapsed Time: 0:00:00|ETA:   0:00:00||\n",
      "Traceback (most recent call last):\n",
      "Episode    1|Iteration 6|reward:    -9.0|last_reward_at:  2|done_at: --|epsilon:   0.9|best_eval_mean: -1.79769e+308|Elapsed Time: 0:00:00|ETA:   0:00:05||\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the Deep Q-learning agent\n",
    "\n",
    "os.makedirs(os.path.join(log_dir, 'training'), exist_ok=True) if log_results else ''\n",
    "env_config = json.loads(json.dumps(dotenv_values()))\n",
    "if configuration.log_results:\n",
    "    with open(os.path.join(log_dir, 'training', '.env.data.yml'), 'w') as outfile:\n",
    "        yaml.dump(env_config, outfile, default_flow_style=False)\n",
    "    with open(os.path.join(log_dir, 'training', '.env.data'), 'w') as outfile:\n",
    "        outfile.write(\"\\n\".join(k + \"=\" + str(v) for k, v in env_config.items()))\n",
    "    logger.info(f\"Loading env variables!\\n{str(env_config)}\")\n",
    "\n",
    "\n",
    "learning_rate = 0.01  # 0.01\n",
    "gamma = 0.015  # 0.015\n",
    "dqn_learning_run = learner.epsilon_greedy_search(\n",
    "    cyberbattle_gym_env=ctf_env,\n",
    "    environment_properties=ep,\n",
    "    learner=dqla.DeepQLearnerPolicy(\n",
    "        ep=ep,\n",
    "        gamma=gamma,\n",
    "        replay_memory_size=10000,\n",
    "        target_update=5,\n",
    "        batch_size=512,  # TODO increase?\n",
    "        learning_rate=learning_rate,  # torch default learning rate is 1e-2\n",
    "        train_while_exploit=train_while_exploit\n",
    "    ),\n",
    "    episode_count=training_episode_count,\n",
    "    iteration_count=iteration_count,\n",
    "    epsilon=0.90,\n",
    "    epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "    epsilon_minimum=0.10,\n",
    "    eval_episode_count=eval_episode_count,\n",
    "    eval_freq=eval_freq,\n",
    "    mean_reward_window=mean_reward_window,\n",
    "    verbosity=Verbosity.Quiet,\n",
    "    render=False,\n",
    "    render_last_episode_rewards_to=os.path.join(log_dir, 'training') if log_results else None,\n",
    "    plot_episodes_length=False,\n",
    "    title=\"DQL\",\n",
    "    save_model_filename=log_results * os.path.join(log_dir, 'training',\n",
    "                                                   f\"{exploit_train}_te{training_episode_count}.tar\")\n",
    ")\n",
    "\n",
    "if log_results:\n",
    "    configuration.writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bd20a02",
   "metadata": {
    "lines_to_next_cell": 0,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# initialize the environment\n",
    "\n",
    "# current_o = ctf_env_2.reset()\n",
    "# wrapped_env = AgentWrapper(ctf_env_2, ActionTrackingStateAugmentation(ep, current_o))\n",
    "DQL_agent = dqn_learning_run['learner']\n",
    "logger.setLevel(logging.INFO) if log_results else ''\n",
    "\n",
    "if log_results:\n",
    "    logger.info(\"Saving model to directory \" + log_dir)\n",
    "    DQL_agent.save(os.path.join(log_dir, f\"{exploit_train}_te{training_episode_count}_final.tar\"))\n",
    "\n",
    "\n",
    "logger.info(\"\")\n",
    "logger.info(\"Now evaluate trained network\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3468569b",
   "metadata": {
    "lines_to_next_cell": 0,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use the trained agent to run the steps one by one\n",
    "\n",
    "max_steps = iteration_count\n",
    "verbosity = Verbosity.Normal\n",
    "DQL_agent.load_best(os.path.join(log_dir, 'training'))\n",
    "DQL_agent.train_while_exploit = train_while_exploit\n",
    "DQL_agent.policy_net.eval()\n",
    "\n",
    "current_o = ctf_env.reset()\n",
    "wrapped_env = AgentWrapper(ctf_env, ActionTrackingStateAugmentation(ep, current_o))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81eab4b9",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Evaluate DQL agent 10 times\n",
    "for n_trial in range(10):\n",
    "    # next action suggested by DQL agent\n",
    "    h = []\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "    df = None\n",
    "    current_o = wrapped_env.reset()\n",
    "    for i in range(max_steps):\n",
    "        logger.info(f\"Step {i}\")\n",
    "        if done:\n",
    "            break\n",
    "        # run the suggested action\n",
    "        action_style, next_action, _ = DQL_agent.exploit(wrapped_env, current_o)\n",
    "\n",
    "        if next_action is None:\n",
    "            logger.info(f\"Inference ended with error: next action == None, returned with aciton_style {action_style}\")\n",
    "            break\n",
    "        current_o, reward, done, info = wrapped_env.step(next_action)\n",
    "        total_reward += reward\n",
    "        action_str, reward_str = wrapped_env.internal_action_to_pretty_print(next_action, output_reward_str=True)\n",
    "        h.append((i,  # wrapped_env.get_explored_network_node_properties_bitmap_as_numpy(current_o),\n",
    "                  reward, total_reward,\n",
    "                  action_str, action_style, info['precondition_str'], info['profile_str'], info[\"reward_string\"]))  # \"\\t action  validity: \" +\n",
    "\n",
    "        df = pd.DataFrame(h, columns=[\"Step\", \"Reward\", \"Cumulative Reward\", \"Next action\", \"Processed by\", \"Precondition\", \"Profile\", \"Reward string\"])\n",
    "        df.set_index(\"Step\", inplace=True)\n",
    "        if log_results:\n",
    "            df.to_csv(os.path.join(log_dir, f'{exploit_train}_evaln{n_trial}_te{training_episode_count}_actions.csv'))\n",
    "\n",
    "    print(f'len: {len(h)}, total reward: {total_reward}')\n",
    "    pd.set_option(\"max_colwidth\", 10**3)\n",
    "    if df is not None:\n",
    "        display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fbbb52f",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": [],
    "title": "if not log_results else 'human'w"
   },
   "outputs": [],
   "source": [
    "    wrapped_env.render(mode='rgb_array', filename=None if not log_results else\n",
    "                       os.path.join(log_dir, f'{exploit_train}_evaln{n_trial}_te{training_episode_count}_network.png'))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,title,-all",
   "cell_metadata_json": true,
   "kernelspec": {
    "display_name": "python3",
    "language": "python",
    "name": "python3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": null,
   "end_time": null,
   "environment_variables": {},
   "exception": null,
   "input_path": "-",
   "output_path": "notebooks/notebook_dql_debug_tiny_ht_one_by_one.ipynb",
   "parameters": {
    "gymid": "CyberBattleTinyMicro-v3",
    "training_episode_count": 2000
   },
   "start_time": "2023-02-01T17:16:11.707573",
   "version": "2.3.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
