{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a08bf6e6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-09T15:39:14.533593Z",
     "iopub.status.busy": "2023-04-09T15:39:14.532757Z"
    },
    "lines_to_next_cell": 2,
    "papermill": {
     "duration": 1.014705,
     "end_time": "2023-04-09T15:39:15.536948",
     "exception": false,
     "start_time": "2023-04-09T15:39:14.522243",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # %%\n",
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "\"\"\"Notebook used for debugging purpose to train the\n",
    "the DQL agent and then run it one step at a time.\n",
    "\"\"\"\n",
    "\n",
    "# pylint: disable=invalid-name\n",
    "\n",
    "# # %%\n",
    "import os\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "import cyberbattle.agents.baseline.agent_wrapper as w\n",
    "import cyberbattle.agents.baseline.agent_dql as dqla\n",
    "from cyberbattle.simulation.actions import Reward, Penalty\n",
    "import logging\n",
    "from cyberbattle.agents.baseline.agent_wrapper import ActionTrackingStateAugmentation, AgentWrapper, Verbosity\n",
    "from IPython.display import display\n",
    "import gym\n",
    "import yaml\n",
    "import json\n",
    "from cyberbattle.simulation.config import configuration, logger\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadccb22",
   "metadata": {
    "lines_to_next_cell": 0,
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "max_episode_steps = 50\n",
    "log_results = os.getenv(\"LOG_RESULTS\", 'False').lower() in ('true', '1', 't')\n",
    "gymid = os.getenv(\"GYMID\", 'CyberBattleTinyMicro-v0')\n",
    "log_level = os.getenv('LOG_LEVEL', \"info\")\n",
    "iteration_count = None\n",
    "honeytokens_on = None\n",
    "training_episode_count = None\n",
    "train_while_exploit = os.getenv(\"TRAIN_WHILE_EXPLOIT\", 'True').lower() in ('true', '1', 't')\n",
    "reward_clip = os.getenv(\"REWARD_CLIP\", 'False').lower() in ('true', '1', 't')\n",
    "eval_episode_count = int(os.getenv('EVAL_EPISODE_COUNT', 0))\n",
    "eval_freq = int(os.getenv('EVAL_FREQ', 0))\n",
    "epsilon_exponential_decay = int(os.getenv('EPS_EXP_DECAY', 2000))  # 5000\n",
    "mean_reward_window = int(os.getenv('MEAN_REWARD_WINDOW', 10))\n",
    "papermill_as_main = False\n",
    "only_eval_summary = False\n",
    "# Algorithm specific parameters\n",
    "learning_rate = 0.001  # 0.01\n",
    "gamma = float(os.getenv('GAMMA', 0.5))  # 0.015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2491f961",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "gymid = \"CyberBattleTinyMicro-v123\"\n",
    "training_episode_count = 2000\n",
    "reward_clip = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa2ea55",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "iteration_count = max_episode_steps if iteration_count is None else iteration_count\n",
    "os.environ['TRAINING_EPISODE_COUNT'] = os.getenv('TRAINING_EPISODE_COUNT', 1000) if training_episode_count is None else str(training_episode_count)\n",
    "training_episode_count = int(os.environ['TRAINING_EPISODE_COUNT'])\n",
    "\n",
    "log_dir = '/logs/exper/' + \"notebook_dql_debug_with_tinymicro\"\n",
    "# convert the datetime object to string of specific format\n",
    "datetime_str = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_dir = os.path.join(log_dir, gymid, datetime_str)\n",
    "os.environ['LOG_DIR'] = log_dir\n",
    "\n",
    "os.environ['LOG_RESULTS'] = str(log_results).lower()\n",
    "exploit_train = \"exploittrain\" * train_while_exploit + \"exploitinfer\" * (1 - train_while_exploit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b3527b",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(gymid=gymid, training_episode_count=training_episode_count,\n",
    "         eval_episode_count=eval_episode_count, iteration_count=iteration_count, epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "         reward_clip=reward_clip, args=None):\n",
    "    if args is not None:\n",
    "        training_episode_count = args.training_episode_count\n",
    "        eval_episode_count = args.eval_episode_count\n",
    "        iteration_count = args.iteration_count\n",
    "        gymid = args.gymid\n",
    "        reward_clip = args.reward_clip\n",
    "        epsilon_exponential_decay = args.eps_exp_decay\n",
    "\n",
    "    os.makedirs(log_dir, exist_ok=True) if log_results else ''\n",
    "    configuration.update_globals(log_dir, gymid, log_level, log_results)\n",
    "    configuration.update_logger()\n",
    "\n",
    "    # if os.environ['RUN_IN_SILENT_MODE'] in ['true']:\n",
    "    #     f = open(os.devnull, 'w')\n",
    "    #     sys.stdout = f\n",
    "\n",
    "    # progressbar.streams.wrap_stderr()\n",
    "\n",
    "    # # %%\n",
    "    ctf_env = gym.make(gymid)\n",
    "    ep = w.EnvironmentBounds.of_identifiers(\n",
    "        maximum_node_count=ctf_env.bounds.maximum_node_count,  # either we identify from configuration, or by ourselves\n",
    "        maximum_total_credentials=1,\n",
    "        identifiers=ctf_env.identifiers\n",
    "    )\n",
    "\n",
    "    ctf_env = gym.make(gymid, env_bounds=ep)\n",
    "    ctf_env.spec.max_episode_steps = max_episode_steps\n",
    "\n",
    "    # if not log_results:\n",
    "    #     lhStdout = logger.handlers[0]  # stdout is the only handler initially\n",
    "    #     logger.removeHandler(lhStdout)\n",
    "    # # %%\n",
    "    # Evaluate the Deep Q-learning agent\n",
    "\n",
    "    os.makedirs(os.path.join(log_dir, 'training'), exist_ok=True) if log_results else ''\n",
    "    env_config = json.loads(json.dumps(dotenv_values()))\n",
    "    if configuration.log_results:\n",
    "        with open(os.path.join(log_dir, 'training', '.env.data.yml'), 'w') as outfile:\n",
    "            yaml.dump(env_config, outfile, default_flow_style=False)\n",
    "        with open(os.path.join(log_dir, 'training', '.env.data'), 'w') as outfile:\n",
    "            outfile.write(\"\\n\".join(k + \"=\" + str(v) for k, v in env_config.items()))\n",
    "        logger.info(f\"Loading env variables!\\n{str(env_config)}\")\n",
    "\n",
    "    # We need to sample with  actions.__process_outcome() but TBH its simpler to assume we have min = minimum(Penalty + Penalty.REPEAT) & max = WinningReward\n",
    "    reward_clip_tuple = None if not reward_clip else (min([Penalty.REPEAT + int(value) for _, value in vars(Penalty).items() if isinstance(value, int) or isinstance(value, float)]),\n",
    "                                                      max([int(value) for _, value in vars(Reward).items() if isinstance(value, int) or isinstance(value, float)]))\n",
    "    if reward_clip:\n",
    "        logger.info(\"Make a reward_clipping to [-1, 1]\")\n",
    "    dqn_learning_run = learner.epsilon_greedy_search(\n",
    "        cyberbattle_gym_env=ctf_env,\n",
    "        environment_properties=ep,\n",
    "        learner=dqla.DeepQLearnerPolicy(\n",
    "            ep=ep,\n",
    "            gamma=gamma,\n",
    "            replay_memory_size=10000,\n",
    "            target_update=5,\n",
    "            batch_size=512,  # TODO increase?\n",
    "            learning_rate=learning_rate,  # torch default learning rate is 1e-2\n",
    "            train_while_exploit=train_while_exploit,\n",
    "            reward_clip=reward_clip_tuple\n",
    "        ),\n",
    "        episode_count=training_episode_count,\n",
    "        iteration_count=iteration_count,\n",
    "        epsilon=0.90,\n",
    "        epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "        epsilon_minimum=0.10,\n",
    "        eval_episode_count=eval_episode_count,\n",
    "        eval_freq=eval_freq,\n",
    "        mean_reward_window=mean_reward_window,\n",
    "        verbosity=Verbosity.Quiet,\n",
    "        render=False,\n",
    "        render_last_episode_rewards_to=os.path.join(log_dir, 'training') if log_results else None,\n",
    "        plot_episodes_length=False,\n",
    "        title=\"DQL\",\n",
    "        only_eval_summary=only_eval_summary,\n",
    "        save_model_filename=log_results * os.path.join(log_dir, 'training',\n",
    "                                                       f\"{exploit_train}_te{training_episode_count}.tar\")\n",
    "    )\n",
    "\n",
    "    if log_results:\n",
    "        configuration.writer.close()\n",
    "    # # %%\n",
    "    # initialize the environment\n",
    "\n",
    "    # current_o = ctf_env_2.reset()\n",
    "    # wrapped_env = AgentWrapper(ctf_env_2, ActionTrackingStateAugmentation(ep, current_o))\n",
    "    dql_agent = dqn_learning_run['learner']\n",
    "    logger.setLevel(logging.INFO) if log_results else ''\n",
    "\n",
    "    if log_results:\n",
    "        logger.info(\"Saving model to directory \" + log_dir)\n",
    "        dql_agent.save(os.path.join(log_dir, f\"{exploit_train}_te{training_episode_count}_final.tar\"))\n",
    "\n",
    "    logger.info(\"\")\n",
    "    logger.info(\"Now evaluate trained network\")\n",
    "    # # %%\n",
    "    # Use the trained agent to run the steps one by one\n",
    "\n",
    "    max_steps = iteration_count\n",
    "    # verbosity = Verbosity.Normal\n",
    "    dql_agent.load_best(os.path.join(log_dir, 'training'))\n",
    "    dql_agent.train_while_exploit = train_while_exploit\n",
    "    dql_agent.policy_net.eval()\n",
    "\n",
    "    current_o = ctf_env.reset()\n",
    "    wrapped_env = AgentWrapper(ctf_env, ActionTrackingStateAugmentation(ep, current_o))\n",
    "    # # %%\n",
    "    # Evaluate DQL agent 10 times\n",
    "    for n_trial in range(10):\n",
    "        # next action suggested by DQL agent\n",
    "        h = []\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        df = None\n",
    "        current_o = wrapped_env.reset()\n",
    "        for i in range(max_steps):\n",
    "            logger.info(f\"Step {i}\")\n",
    "            if done:\n",
    "                break\n",
    "            # run the suggested action\n",
    "            action_style, next_action, _ = dql_agent.exploit(wrapped_env, current_o)\n",
    "\n",
    "            if next_action is None:\n",
    "                logger.info(f\"Inference ended with error: next action == None, returned with aciton_style {action_style}\")\n",
    "                break\n",
    "            current_o, reward, done, info = wrapped_env.step(next_action)\n",
    "            total_reward += reward\n",
    "            action_str, reward_str = wrapped_env.internal_action_to_pretty_print(next_action, output_reward_str=True)\n",
    "            h.append((i,  # wrapped_env.get_explored_network_node_properties_bitmap_as_numpy(current_o),\n",
    "                      reward, total_reward,\n",
    "                      action_str, action_style, info['precondition_str'], info['profile_str'], info[\"reward_string\"]))  # \"\\t action  validity: \" +\n",
    "\n",
    "            df = pd.DataFrame(h, columns=[\"Step\", \"Reward\", \"Cumulative Reward\", \"Next action\", \"Processed by\", \"Precondition\", \"Profile\", \"Reward string\"])\n",
    "            df.set_index(\"Step\", inplace=True)\n",
    "            if log_results:\n",
    "                df.to_csv(os.path.join(log_dir, f'{exploit_train}_evaln{n_trial}_te{training_episode_count}_actions.csv'))\n",
    "\n",
    "        print(f'len: {len(h)}, total reward: {total_reward}')\n",
    "        pd.set_option(\"max_colwidth\", 10**3)\n",
    "        if df is not None:\n",
    "            display(df)\n",
    "\n",
    "        # if not log_results else 'human'w\n",
    "        wrapped_env.render(mode='rgb_array', filename=None if not log_results else\n",
    "                           os.path.join(log_dir, f'{exploit_train}_evaln{n_trial}_te{training_episode_count}_network.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4d60df",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if papermill_as_main:\n",
    "    main()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,title,-all",
   "cell_metadata_json": true,
   "kernelspec": {
    "display_name": "python3",
    "language": "python",
    "name": "python3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2.491025,
   "end_time": "2023-04-09T15:39:15.710756",
   "environment_variables": {},
   "exception": null,
   "input_path": "-",
   "output_path": "notebooks/notebook_dql_debug_tiny_ht_gradual_increase.ipynb",
   "parameters": {
    "gymid": "CyberBattleTinyMicro-v123",
    "reward_clip": true,
    "training_episode_count": 2000
   },
   "start_time": "2023-04-09T15:39:13.219731",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}