{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eeb550ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-14T22:40:22.294262Z",
     "iopub.status.busy": "2023-02-14T22:40:22.293667Z",
     "iopub.status.idle": "2023-02-14T22:40:24.456011Z",
     "shell.execute_reply": "2023-02-14T22:40:24.455263Z"
    },
    "lines_to_next_cell": 2,
    "papermill": {
     "duration": 2.177054,
     "end_time": "2023-02-14T22:40:24.461666",
     "exception": false,
     "start_time": "2023-02-14T22:40:22.284612",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # %%\n",
    "# Copyright (c) Microsoft Corporation.\n",
    "# Licensed under the MIT License.\n",
    "\n",
    "\"\"\"Notebook used for debugging purpose to train the\n",
    "the DQL agent and then run it one step at a time.\n",
    "\"\"\"\n",
    "\n",
    "# pylint: disable=invalid-name\n",
    "\n",
    "# # %%\n",
    "import os\n",
    "from dotenv import load_dotenv, dotenv_values\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import cyberbattle.agents.baseline.learner as learner\n",
    "import cyberbattle.agents.baseline.agent_wrapper as w\n",
    "import cyberbattle.agents.baseline.agent_dql as dqla\n",
    "import logging\n",
    "from cyberbattle.agents.baseline.agent_wrapper import ActionTrackingStateAugmentation, AgentWrapper, Verbosity\n",
    "from IPython.display import display\n",
    "import gym\n",
    "import yaml\n",
    "import json\n",
    "from cyberbattle.simulation.config import configuration, logger\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95d419b3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-14T22:40:24.472738Z",
     "iopub.status.busy": "2023-02-14T22:40:24.472089Z",
     "iopub.status.idle": "2023-02-14T22:40:24.482236Z",
     "shell.execute_reply": "2023-02-14T22:40:24.481574Z"
    },
    "lines_to_next_cell": 0,
    "papermill": {
     "duration": 0.017427,
     "end_time": "2023-02-14T22:40:24.485124",
     "exception": false,
     "start_time": "2023-02-14T22:40:24.467697",
     "status": "completed"
    },
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "max_episode_steps = 50\n",
    "log_results = os.getenv(\"LOG_RESULTS\", 'False').lower() in ('true', '1', 't')\n",
    "gymid = os.getenv(\"GYMID\", 'CyberBattleTinyMicro-v0')\n",
    "log_level = os.getenv('LOG_LEVEL', \"info\")\n",
    "iteration_count = None\n",
    "honeytokens_on = None\n",
    "training_episode_count = None\n",
    "train_while_exploit = os.getenv(\"TRAIN_WHILE_EXPLOIT\", 'True').lower() in ('true', '1', 't')\n",
    "eval_episode_count = int(os.getenv('EVAL_EPISODE_COUNT', 0))\n",
    "eval_freq = int(os.getenv('EVAL_FREQ', 0))\n",
    "epsilon_exponential_decay = int(os.getenv('EPS_EXP_DECAY', max_episode_steps * 4000))  # 5000\n",
    "mean_reward_window = int(os.getenv('MEAN_REWARD_WINDOW', 10))\n",
    "papermill_as_main = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "938ffdcb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-14T22:40:24.493918Z",
     "iopub.status.busy": "2023-02-14T22:40:24.493521Z",
     "iopub.status.idle": "2023-02-14T22:40:24.497952Z",
     "shell.execute_reply": "2023-02-14T22:40:24.497203Z"
    },
    "papermill": {
     "duration": 0.01169,
     "end_time": "2023-02-14T22:40:24.500608",
     "exception": false,
     "start_time": "2023-02-14T22:40:24.488918",
     "status": "completed"
    },
    "tags": [
     "injected-parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "__name__ = \"__main__\"\n",
    "gymid = \"CyberBattleTinyMicro-v2\"\n",
    "training_episode_count = 2000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11a9f575",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-14T22:40:24.508361Z",
     "iopub.status.busy": "2023-02-14T22:40:24.507806Z",
     "iopub.status.idle": "2023-02-14T22:40:24.518259Z",
     "shell.execute_reply": "2023-02-14T22:40:24.517524Z"
    },
    "lines_to_next_cell": 1,
    "papermill": {
     "duration": 0.017706,
     "end_time": "2023-02-14T22:40:24.521131",
     "exception": false,
     "start_time": "2023-02-14T22:40:24.503425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "iteration_count = max_episode_steps if iteration_count is None else iteration_count\n",
    "os.environ['TRAINING_EPISODE_COUNT'] = os.getenv('TRAINING_EPISODE_COUNT', 1000) if training_episode_count is None else str(training_episode_count)\n",
    "training_episode_count = int(os.environ['TRAINING_EPISODE_COUNT'])\n",
    "\n",
    "log_dir = '/logs/exper/' + \"notebook_dql_debug_with_tinymicro\"\n",
    "# convert the datetime object to string of specific format\n",
    "datetime_str = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_dir = os.path.join(log_dir, gymid, datetime_str)\n",
    "os.environ['LOG_DIR'] = log_dir\n",
    "\n",
    "os.environ['LOG_RESULTS'] = str(log_results).lower()\n",
    "exploit_train = \"exploittrain\" * train_while_exploit + \"exploitinfer\" * (1 - train_while_exploit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2958a33c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-14T22:40:24.530209Z",
     "iopub.status.busy": "2023-02-14T22:40:24.529887Z",
     "iopub.status.idle": "2023-02-14T22:40:24.564753Z",
     "shell.execute_reply": "2023-02-14T22:40:24.564105Z"
    },
    "papermill": {
     "duration": 0.044306,
     "end_time": "2023-02-14T22:40:24.569405",
     "exception": false,
     "start_time": "2023-02-14T22:40:24.525099",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def main(gymid, training_episode_count, eval_episode_count, iteration_count, args=None):\n",
    "    if args is not None:\n",
    "        training_episode_count = args.training_episode_count\n",
    "        eval_episode_count = args.eval_episode_count\n",
    "        iteration_count = args.iteration_count\n",
    "        gymid = args.gymid\n",
    "\n",
    "    os.makedirs(log_dir, exist_ok=True) if log_results else ''\n",
    "    configuration.update_globals(log_dir, gymid, log_level, log_results)\n",
    "    configuration.update_logger()\n",
    "\n",
    "    # if os.environ['RUN_IN_SILENT_MODE'] in ['true']:\n",
    "    #     f = open(os.devnull, 'w')\n",
    "    #     sys.stdout = f\n",
    "\n",
    "    # progressbar.streams.wrap_stderr()\n",
    "\n",
    "    # # %%\n",
    "    ctf_env = gym.make(gymid)\n",
    "    ep = w.EnvironmentBounds.of_identifiers(\n",
    "        maximum_node_count=ctf_env.bounds.maximum_node_count,  # either we identify from configuration, or by ourselves\n",
    "        maximum_total_credentials=1,\n",
    "        identifiers=ctf_env.identifiers\n",
    "    )\n",
    "\n",
    "    ctf_env = gym.make(gymid, env_bounds=ep)\n",
    "    ctf_env.spec.max_episode_steps = max_episode_steps\n",
    "\n",
    "    # if not log_results:\n",
    "    #     lhStdout = logger.handlers[0]  # stdout is the only handler initially\n",
    "    #     logger.removeHandler(lhStdout)\n",
    "    # # %%\n",
    "    # Evaluate the Deep Q-learning agent\n",
    "\n",
    "    os.makedirs(os.path.join(log_dir, 'training'), exist_ok=True) if log_results else ''\n",
    "    env_config = json.loads(json.dumps(dotenv_values()))\n",
    "    if configuration.log_results:\n",
    "        with open(os.path.join(log_dir, 'training', '.env.data.yml'), 'w') as outfile:\n",
    "            yaml.dump(env_config, outfile, default_flow_style=False)\n",
    "        with open(os.path.join(log_dir, 'training', '.env.data'), 'w') as outfile:\n",
    "            outfile.write(\"\\n\".join(k + \"=\" + str(v) for k, v in env_config.items()))\n",
    "        logger.info(f\"Loading env variables!\\n{str(env_config)}\")\n",
    "\n",
    "    learning_rate = 0.01  # 0.01\n",
    "    gamma = 0.015  # 0.015\n",
    "    dqn_learning_run = learner.epsilon_greedy_search(\n",
    "        cyberbattle_gym_env=ctf_env,\n",
    "        environment_properties=ep,\n",
    "        learner=dqla.DeepQLearnerPolicy(\n",
    "            ep=ep,\n",
    "            gamma=gamma,\n",
    "            replay_memory_size=10000,\n",
    "            target_update=5,\n",
    "            batch_size=512,  # TODO increase?\n",
    "            learning_rate=learning_rate,  # torch default learning rate is 1e-2\n",
    "            train_while_exploit=train_while_exploit\n",
    "        ),\n",
    "        episode_count=training_episode_count,\n",
    "        iteration_count=iteration_count,\n",
    "        epsilon=0.90,\n",
    "        epsilon_exponential_decay=epsilon_exponential_decay,\n",
    "        epsilon_minimum=0.10,\n",
    "        eval_episode_count=eval_episode_count,\n",
    "        eval_freq=eval_freq,\n",
    "        mean_reward_window=mean_reward_window,\n",
    "        verbosity=Verbosity.Quiet,\n",
    "        render=False,\n",
    "        render_last_episode_rewards_to=os.path.join(log_dir, 'training') if log_results else None,\n",
    "        plot_episodes_length=False,\n",
    "        title=\"DQL\",\n",
    "        save_model_filename=log_results * os.path.join(log_dir, 'training',\n",
    "                                                       f\"{exploit_train}_te{training_episode_count}.tar\")\n",
    "    )\n",
    "\n",
    "    if log_results:\n",
    "        configuration.writer.close()\n",
    "    # # %%\n",
    "    # initialize the environment\n",
    "\n",
    "    # current_o = ctf_env_2.reset()\n",
    "    # wrapped_env = AgentWrapper(ctf_env_2, ActionTrackingStateAugmentation(ep, current_o))\n",
    "    DQL_agent = dqn_learning_run['learner']\n",
    "    logger.setLevel(logging.INFO) if log_results else ''\n",
    "\n",
    "    if log_results:\n",
    "        logger.info(\"Saving model to directory \" + log_dir)\n",
    "        DQL_agent.save(os.path.join(log_dir, f\"{exploit_train}_te{training_episode_count}_final.tar\"))\n",
    "\n",
    "    logger.info(\"\")\n",
    "    logger.info(\"Now evaluate trained network\")\n",
    "    # # %%\n",
    "    # Use the trained agent to run the steps one by one\n",
    "\n",
    "    max_steps = iteration_count\n",
    "    verbosity = Verbosity.Normal\n",
    "    DQL_agent.load_best(os.path.join(log_dir, 'training'))\n",
    "    DQL_agent.train_while_exploit = train_while_exploit\n",
    "    DQL_agent.policy_net.eval()\n",
    "\n",
    "    current_o = ctf_env.reset()\n",
    "    wrapped_env = AgentWrapper(ctf_env, ActionTrackingStateAugmentation(ep, current_o))\n",
    "    # # %%\n",
    "    # Evaluate DQL agent 10 times\n",
    "    for n_trial in range(10):\n",
    "        # next action suggested by DQL agent\n",
    "        h = []\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        df = None\n",
    "        current_o = wrapped_env.reset()\n",
    "        for i in range(max_steps):\n",
    "            logger.info(f\"Step {i}\")\n",
    "            if done:\n",
    "                break\n",
    "            # run the suggested action\n",
    "            action_style, next_action, _ = DQL_agent.exploit(wrapped_env, current_o)\n",
    "\n",
    "            if next_action is None:\n",
    "                logger.info(f\"Inference ended with error: next action == None, returned with aciton_style {action_style}\")\n",
    "                break\n",
    "            current_o, reward, done, info = wrapped_env.step(next_action)\n",
    "            total_reward += reward\n",
    "            action_str, reward_str = wrapped_env.internal_action_to_pretty_print(next_action, output_reward_str=True)\n",
    "            h.append((i,  # wrapped_env.get_explored_network_node_properties_bitmap_as_numpy(current_o),\n",
    "                      reward, total_reward,\n",
    "                      action_str, action_style, info['precondition_str'], info['profile_str'], info[\"reward_string\"]))  # \"\\t action  validity: \" +\n",
    "\n",
    "            df = pd.DataFrame(h, columns=[\"Step\", \"Reward\", \"Cumulative Reward\", \"Next action\", \"Processed by\", \"Precondition\", \"Profile\", \"Reward string\"])\n",
    "            df.set_index(\"Step\", inplace=True)\n",
    "            if log_results:\n",
    "                df.to_csv(os.path.join(log_dir, f'{exploit_train}_evaln{n_trial}_te{training_episode_count}_actions.csv'))\n",
    "\n",
    "        print(f'len: {len(h)}, total reward: {total_reward}')\n",
    "        pd.set_option(\"max_colwidth\", 10**3)\n",
    "        if df is not None:\n",
    "            display(df)\n",
    "\n",
    "        # if not log_results else 'human'w\n",
    "        wrapped_env.render(mode='rgb_array', filename=None if not log_results else\n",
    "                           os.path.join(log_dir, f'{exploit_train}_evaln{n_trial}_te{training_episode_count}_network.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c92885b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-02-14T22:40:24.577988Z",
     "iopub.status.busy": "2023-02-14T22:40:24.577521Z",
     "iopub.status.idle": "2023-02-14T23:05:03.851774Z",
     "shell.execute_reply": "2023-02-14T23:05:03.851152Z"
    },
    "papermill": {
     "duration": 1478.604956,
     "end_time": "2023-02-14T23:05:03.178342",
     "exception": false,
     "start_time": "2023-02-14T22:40:24.573386",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if papermill_as_main:\n",
    "    main(gymid, training_episode_count, eval_episode_count, iteration_count)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(gymid, training_episode_count, eval_episode_count, iteration_count)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "tags,title,-all",
   "cell_metadata_json": true,
   "kernelspec": {
    "display_name": "python3",
    "language": "python",
    "name": "python3"
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1484.214659,
   "end_time": "2023-02-14T23:05:04.625133",
   "environment_variables": {},
   "exception": null,
   "input_path": "-",
   "output_path": "notebooks/notebook_dql_debug_tiny_ht_v2_redo.ipynb",
   "parameters": {
    "__name__": "__main__",
    "gymid": "CyberBattleTinyMicro-v2",
    "training_episode_count": 2000
   },
   "start_time": "2023-02-14T22:40:20.410474",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}